# Classification Evaluation

The training loss dropping quickly below the validation loss on Figure \ref{fig:lossgraph} (A) likely identifies a potential issue with the network, suggesting that even at a very low learning rate, within one epoch the model is slightly over-fit to the training data. This is again emphasised by the F1 score of 1.0 shown on Figure \ref{fig:lossgraph} (B), that is both above the validation F1 and suggests the model perfectly predicts flood related Tweets, a strong indication that the model is over-fit. Despite this however, the high validation F1 score does suggest that despite the model being over-fit, it is likely that it will perform well, generalising well with unseen data.

One particular issue with this model is that despite the good performance on unseen validation data, the metrics shown on Table \ref{tab:evaltable} and Figure \ref{fig:confplot} are far lower. This issue suggests that it is likely that there is simply not enough labelled data. Tweets themselves are very variable, using unstructured language which often contains misspellings and inconsistent capitalisation. With only a selection of around 500 Tweets selected for testing, 1800 for training, and 200 for validation, the model is unlikely to capture enough information to generalise well enough to perform consistently on unseen data. Despite this however, it should be noted that the model still outperforms the rule-based approach, meaning even with such a small amount of labelled data, the model does capture relevant semantic information automatically. The final issue that is highlighted by this discrepancy is a general issue with supervised learning, especially when considering text-based information. A distinct difference in testing performance against a validation performance suggests that labelling requirements may have been subtly different. This reflects the ambiguity that is often encountered when attempting to assign binary labels to short text. Qualitatively this was observed during annotation, for example the Tweet:

> 'Why do people leave their brains behind when they get behind the wheel of a car? surely the police have enough to do https://t.co/PnWooKQzVF'

At first glance, this Tweet appears unrelated to flooding and as such would be given a negative classification. However, knowing that this Tweet occurred during a flooding event, within the boundaries of the flood zone affected, it then seems more likely than not that police involvement is very likely related to the flood event itself. This Tweet also highlights another issue with the use of a purely text-based model for this task. Viewing the image attached to the Tweet reveals a news article that describing motorists ignoring police barricades to prevent flooded roads from being accessed. This appears to reflect a more emotive response that often comes with Tweets relating to emergency events. While people do often Tweet more when experiencing incidents, their language is more often ambiguous and in a more complex form, which is more difficult to quantitatively assess [@umihara2013].

This leads to the primary issue with this classification task. Binary annotation of Tweets into flood related and unrelated is often complicated by ambiguity. This has been noted in past related work, for example, @olteanu2015 note that attempting to classify 'informative' Tweets against 'non-informative' Tweets, is a difficult task, even for human annotators. This is also emphasised on Twitter due to the large amount of informal language, which leads to ambiguity when semantics are unclear [@chen2014].

There are two considerations that may improve this problem, firstly, it is likely beneficial to consider a more structured definition when determining whether a Tweet is flood related or not. Studies that have considered multi-class classification may find this simpler, selecting classes that are less ambiguous, for example _missing persons_, _property damage_. This would mean that both human annotators, and the model, have more defined targets for labelling, with less ambiguity when a Tweet relates to a flood in a more tangential way. Issues however form if classes are too dilute, limiting the training information available to the model, which would lead to over-fitting. In this dissertation for example, there were no Tweets observed that related to missing persons, most are simply exclamations about the weather, often noting the broad areas affected. The other consideration is the use of multiple annotators, which is common in NLP tasks [@beck2020]. This would highlight instances where ambiguity is common, making them easier to resolve, or identify adjustments to be made to annotation guidelines.

After these considerations, the major question is whether to select a _supervised_ classification model or use _unsupervised_ keyword extraction. I have demonstrated that it is possible to gain superior results by choosing a supervised methodology, meaning with tweaks to training data, it may be possible to produce a model that significantly outperforms a rule-based approach.

The model presented also appears to outperform past machine learning methods from many past studies. This is likely a result of both the pre-training procedure used by the Twitter specific language model, enabling semantic links between relevant flood related words to be observed (See Figure \ref{fig:transformerviz}), and the complexity of the transformer model itself, which enables weight updates to quickly learn the appropriate embeddings to match the annotated training data. Notably, \ref{fig:transformerviz} (B) indicates that while lightning is likely considered by the model in most contexts to be associated with floods, the model is able to consider this instance independently, understanding that in this context the word _'lightning'_ is not weather related. The attributions on Table \ref{tab:attributions} mirror keyword selection, showing the model automatically highlights keywords that are important for classification, additionally giving them weightings. This mirrors the manual, time-consuming methods of @saravanou2015 who identify many keywords through observation of a subset of their full corpus.

Overall however, there is still a relatively small difference in the model performance against the rule-based approach, relative to the amount of time required to annotate data for supervised training, and the time required for training. This is also reflected in previous work that considered this comparison, only gaining small improvements using machine learning over rule-based methods for a similar task [@caragea2011].

Due to the variability and ambiguity of this task, it is tempting to suggest there is a higher influence from outside factors when determining performance metrics. Tweets from other flooding events from past studies vary significantly to those observed in my dissertation, keywords from @saravanou2015 for example are often event specific. This influence also comes from the annotation of training data, it is likely that given two separate annotators, the results of the model are likely to vary widely. This may also be applied to keyword classification methods, as past studies have selected a variety of keywords, without a method to formally justify their choices.

The dataset imbalance indicated by Figure \ref{fig:confplot} also contributes to some potentially misleading evaluation metrics reported by similar work. Notably @caragea2016 demonstrate that using a _Na√Øve approach_ to classification of flood related Tweets, i.e. classifying every Tweet as the majority class, gives results approaching an accuracy of just below 70\%, and in some cases only slightly below the results of their SVM models. This also reflects the requirement of using an F1 score as a performance metric over accuracy, as F1 scores are more capable of taking into account class imbalances.

It has been generally demonstrated that the use of deep learning neural networks that take in pre-trained word embeddings enables models to infer their own rules based purely on the raw text and derived semantic contexts. This comes from both the short passage itself, and the data used during the pre-training procedure. This presents an alternative to the _ad hoc_ rules created when selecting keywords, or when engineering features for input into more traditional machine learning models.

# Spatio-temporal Analysis

The analysis regarding place names mentioned in Tweets over the flood events on Figure \ref{fig:dailychange} (B) appears to correlate with past research that has suggested there is often a much larger proportion of geographic information present in Tweets during emergency events [@grace2020a]. The slight up-tick in flood related Tweets mentioned prior to the flood warning day on \ref{fig:dailychange} (A) also suggests that there may be Tweets that report flooding emergencies prior to official warnings, as found with past research [@perng2013;@martinez-rojas2018]. Further analysis would be required to determine whether particular Tweets do explicitly report flooding before it is reported formally. Given _flood related_ Tweets appear even 7 days prior to events, they are not all likely predictive of flooding at that stage.

Further work may consider geo-coding place names from Tweets, to observe the spatial distribution of Tweets during each event [@lorini2019]. However, many gazetteers, popular for geo-coding work, are likely to exclude place names that are more localised or are colloquial [@twaroch2008;@gao2017].

# Conclusion

My dissertation demonstrates the use of a Twitter-based pre-trained transformer language model to classify Tweets relating to flood events as relevant or irrelevant. This model requires no _ad hoc_ feature engineering or keyword selection, meaning outputs are less likely to demonstrate bias derived from this selection of features. However, as demonstrated, the performance appears to be relatively similar to classification through keyword matching, suggesting that use-cases for such models may be situation dependant.

Future work when considering deep learning for Tweet classification should consider the ambiguity that is often present when selecting binary labels. There is a balance between the use of multi-label classification, which in some cases may allow the model to group Tweets that contain a similar context, but could present problems if the categories become too diluted. Labelling ambiguity may be partially resolved by the use of multiple annotators [@beck2020], but given the informal nature of Tweets, this problem is likely to persist.

Recent work has considered the prominent issue with human annotated labelling accuracy in a variety of supervised machine learning methods, which appears on many major datasets [@northcutt2021]^[See https://labelerrors.com/]. Future work may consider the edge cases that occur with low model confidence, and incorrect labels to determine whether there are clear labelling issues.
