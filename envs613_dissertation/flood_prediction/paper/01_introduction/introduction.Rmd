Twitter presents large continuous feed of information regarding emergency events, contributed through individual users, as these events occur. In some cases, the first reports about emergencies on social media often precede those of mainstream media [@perng2013;@martinez-rojas2018;@kim2018;@laylavi2016]. Many emergency events have been studied in relation to Twitter, including hurricanes and floods in the US [@hughes2014;@kim2018], Paris terror attacks in 2015 [@reilly2021], and UK flooding events [@saravanou2015;@brouwer2017].

Extreme weather events have become increasingly common [@kron2019], a trend that is expected to continue into the future [@forzieri2017], meaning there is an increasing demand to predict and understand how natural disasters develop. While environmental emergency events are often predictable through meteorological instruments, they generally provide the most detail in high population areas due their high cost, limiting their real-time availability and spatio-temporal scale in many locations [@muller2015]. This is a particular issue for flooding events, where events are typically a smaller scale and time-frame [@willems2012;@arnbjerg-nielsen2013].

Tweets during emergency events often contain more fine-grained locational information [@grace2020a], at a scale that is unlikely identifiable at real-time through meteorological tools [@mazzoleni2017;@muller2015]. The ability to accurately identify relevant Tweets during flood events presents the opportunity to provide first responders with first-hand information regarding floods as they develop, while ignoring the large amounts of noise that often overpowers the informative information [@ashktorab2014].

Much of the past work that has used Twitter to study past emergency events has used keywords to identify relevant Tweets [@kryvasheyeu2016;@brouwer2017;@morstatter2013]. This however has several issues; keywords are human selected, meaning they require a pre-existing knowledge of the semantics used to describe targeted events. Certain keywords also do not always relate to these emergency events [@sakaki2010;@spielhofer2016], for example a person may be in _'floods of tears'_. Finally, Tweets relating to emergency events also do not necessarily contain an obvious keyword, and therefore are unable to be detected.

More recent work has considered the ability to use machine learning to classify Tweets into those relating to emergency events, and those that are unrelated [@imran2020;@arthur2018;@sakaki2010;@li2018b]. These studies have utilised a variety of methods, building from classical approaches like Na√Øve Bayes classification [@imran2013;@li2018b] and Support Vector Machines [@caragea2011;@sakaki2010], while more recent work has considered the emerging prevalence of neural networks in text-based classification [@caragea2016;@debruijn2020;@caragea2016].

Most analysis of social media data relating to emergency events is retrospective, and work that considers the ability to _predict_ emergency events through social media primarily relates to _event detection_ in literature [@castillo2016]. My dissertation considers the ability to target and classify Tweets relating to past _known_ flooding events in the United Kingdom. This therefore represents a retrospective approach that may be applied to future events in real-time. Such events are already suitably predictable at a general scale using existing meteorological equipment, suggesting pure _event detection_ is not required. This work aims to demonstrate a pipeline that is primarily focussed on the removal of irrelevant information from the Twitter pipeline during ongoing flooding events, providing information that may be of use to emergency responders.

For classification, my work uses a neural network architecture known as a _transformer_ to classify Tweets extracted from past known major flooding events within the United Kingdom. This model was pre-trained using a large amount of Twitter data, allowing for relevant embedded information to be captured prior to additional task-specific training. The transformer architecture proceeds more traditional neural models for text classification like Recurrent Neural Networks (RNNs), by using the attention mechanism to more suitably capture textual context [@vaswani2017].
