The use of Twitter data as a source of information relating to emergency events may be seen as a type of crowd-sourcing. @arthur2018 suggest a distinction between 'solicited' crowd-sourcing, where citizens are encouraged to contribute directly to data collection efforts, and 'unsolicited', where communication between users on web platforms allow for the indirect extraction of important information. While several attempts have been made to encourage direct contributions to monitor extreme weather events [@metoffice;@europeanseverestormslaboratoryessl;@muller2015], there is a heavy reliance on dedicated volunteers choosing to upload data, which is incomparable to the volume of passively contributed data through social media sources. Other forms of 'unsolicited' crowd-sourcing exist for example through call data records (CDR), used to analyse movement patterns in response to natural disasters [@lu2012], these however often exist as commercial data products, with increased privacy concerns.

In emergency situations, traditionally there would have been a reliance on formal media sources to disseminate information, of which only one-way communication is possible, with the information itself only provided by official organisations [@schneider2010]. With the introduction of social media, there is now a source of two-way communication between both citizens and official sources, meaning information propagates more quickly, and often through first-hand experiences. As part of best practices aimed towards emergency legislation, @lin2016 suggest that the use of social media for the two-way communication of information is an important source of information, and two-way communication should be encouraged. As of 2021 there are now over 17 million active Twitter users in the United Kingdom [@statista2021], with a significant number using mobile devices, meaning there are often multiple accounts that are able to communicate specific information regarding emergencies as they occur. The speed of communication through social media is also much faster than traditional sources, and many studies have found that the first messages regarding emergencies situations often appear on social media before traditional news sources [@kim2018;@laylavi2016;@perng2013;@martinez-rojas2018]. Twitter specifically has been noted as a platform that through design allows for a good source of information regarding emergency events due to its speed and ease of use [@simon2015;@williams2013], and first-hand accounts of emergency responders have noted Twitter provides them with the information they need to assist during a crisis [@brengarth2016].

# Studying Emergencies through Twitter

Much of the existing work regarding Twitter and emergencies concentrates on event detection. An event may be described as a significant occurrence of something at a specific time and location [@brants2003]. In social media these are typically characterised by an increase in volume of messages associated with particular topics or entities such as people or places [@dou2012]. With respect to emergencies, @sakaki2010 notes that they are distinguishable by their large scale in which many users are experiencing a particular event, they influence the daily life of a person, and have both spatial and temporal regions. There is also a distinction between retrospective data analysis of emergencies and live data analysis [@castillo2016], and often with live data analysis the event itself is unknown [@brants2003], which @atefeh2015 considered to be _unspecified_ event detection.

While some studies consider the ability to detect _unspecified_ events through live data analysis of social media [@sakaki2010;@dou2012;@pekar2020], @castillo2016 note that for many natural disaster events, the information provided through meteorological observations is likely to be far more informative to predict the initial temporal and spatial information regarding events. However, it is likely that when an event is known, the information provided through relevant social media posts may provide additional fine-grained information about localised incidents as the event progresses, especially as real-time data at a small scale is often unavailable [@mazzoleni2017;@muller2015]. The type of information provided through Twitter is also different; rainfall data and flood prediction do not necessarily provide information relating to the direct impact on peoples lives, whilst Twitter presents information based on first-hand experiences [@muller2015].

For this reason, the following work considers only the retrospective analysis of _specified_ past flooding events. Conceptually this work aims to demonstrate the ability to capture information _during_ known flood events, rather than demonstrate the ability to predict these events, as meteorological tools already do this with good accuracy. This capture of information first relates to the _classification_ of relevant Tweets relating to floods, filtering out large proportion of irrelevant information that simply provides no use to responders. Second, the ability to capture information that is not provided through meteorological tools is explored, considering the fact that events exist primarily in a spatio-temporal dimension. Information extracted through my dissertation therefore primarily concentrates on fine-grained locational information that is often communicated during emergency events [@grace2020a], and which cannot be identified through meteorological tools. Temporal information may be captured through analysis of Tweets classified as relevant, and the information extracted through them as events progress.

# Classification of Flood Tweets

Tweet classification methodologies are grouped broadly into either binary classification, or multi-label classification. Binary classifications generally take a stream of Tweets, identifying those that are _informative_ or _relevant_, removing any that do not provide useful information or relate to the emergency event being targeted [@ghafarian2020]. This classification is essential given relevant information during emergencies is often diluted due to the large general volume of social media platforms [@lin2016]. Multi-label classification typically focusses on Tweets that are already considered to be relevant to the targeted emergency event, categorising them into groups that indicate the type of information being captured, for example Tweets relating to _missing persons_ or _damage_ [@ashktorab2014].

## Building a Corpus

The first stage of Tweet classification is the extraction of the Tweets themselves. Studies that have considered Tweet extraction relating to floods sometimes identify _non-specific_ events, extracting general Tweets using keywords with Twitters Streaming API over large time periods. For example @arthur2018 extracted almost 18 million Tweets over a year using the keywords _'flood, flooding, flooded'_, filtering Tweets that did not use a UK time-zone as an alternative to the reliance on geotags to only retrieve Tweets within the UK. Others have targeted specific major flooding events, @spielhofer2016 were able to target major flooding events in the UK over 2015-2016, filtering using the keywords _'flood(ing), heavy rain, stormy weather'_, amounting to over 970,000 Tweets over 50 days.

One issue with these studies is that they opt to ignore any geographic information relating to Tweets, due to the very low proportion of Tweets actually containing a geotag [~1.5\%, @spielhofer2016]. This decision led to many of the Tweets extracted being unrelated to any targeted flooding event, even when containing the correct keywords. @spielhofer2016 for example found that terms like _'floods of tears'_ were present, and that many of the Tweets extracted that did relate to flooding were outside of their target area. The use of keywords to filter for relevant Tweets also means that many flood related Tweets may be removed that do not necessarily contain the keywords selected. The two papers mentioned for example select two distinctly different criteria for keywords, meaning even given the same time-period, their corpora would differ.

Alternatively, @saravanou2015 specifically targeted Tweets with geotags placing them within a bounding box of the United Kingdom over a 5-day period in 2014 during major flooding events, without filtering for keywords. In total a corpus of 2.3 million geotagged Tweets was built. To extract flood related Tweets, a large collection of 456 flood related keywords was created based on a lexicon derived from an initial subset of the full corpus using a much smaller set of initial keywords.

## Machine Learning Classification

While the stage at which Tweets were classified differed in the above studies, either at the time of collection [@arthur2018;@spielhofer2016], or following collection, after observing the corpus [@saravanou2015], all have relied on simple keyword extraction to classify relevant and irrelevant Tweets. Using keywords leads to relatively _ad hoc_ classifications, based entirely on the authors decision regarding the keywords selected. Other work has considered using more sophisticated methods, using machine learning to classify Tweets relating to emergency events. This work is again split into binary classification of relevant and irrelevant information, and multi-label classification where multiple labels as assigned to Tweets indicating the type of content they include. Rather than pre-selecting keywords, machine learning enables a trained model to select its own _features_ to determine the correct classification of Tweets.

As machine learning models require input variables rather than raw text, there is a reliance on suitable _feature engineering_ to create input variables for certain model types. @sakaki2010 for example generate features using (1) the number of words in a Tweet and the position of a keyword, (2) words within each Tweet represented as an index value, and (3) context surrounding the keywords. With these features they classify Tweets relating to earthquakes or not using a Support Vector Machine (SVM) model. Their evaluation results report an F~1~ score of 73.7\% when using all features, with most coming from feature (1), notably precision is much lower than recall.

Alternatively, classification of Tweets may consider a two phase approach, first identifying emergency related Tweets using keywords, then classifying those Tweets as related to flooding events or not, attempting to reduce the number of false positives. @ashktorab2014 present a pipeline for automatically classifying Tweets extracted using keywords, geographical bounding areas and hashtags relating to past emergency events. For classification, a subset of Tweets were assigned a binary classification, noting whether the content related to the emergency event or not. Several classification algorithms were evaluated in relation to this dataset, taking a standard unigram feature vector as input. Logistic regression gave the best F1 score of ~0.65, but recall was low across all models, suggesting many flood related Tweets were likely missed during classification.

@caragea2011 present a comparison between the use of SVMs and keywords for the classification of text messages relating to the Haiti earthquake into various topics. Various features were used as input for the SVM, including _feature abstractions_, _bag of words_, _topic words_, and _LDA output_, with _feature abstractions_ giving the best results. Notably however, the performance of the SVM machine learning approach only slightly outperformed the results of classification using only keywords.

@ghafarian2020 suggest that methods that place emphasis on certain keywords as a feature input into SVMs, Naive Bayes Classifiers and other machine learning classifiers to identify relevant Tweets likely miss other relevant information. @ghafarian2020 instead consider each Tweet to be a _distribution_ based on word vectors derived from the `Word2Vec` model, suggesting that flood related Tweets will have similar distributions. Using these distributions alongside models like SVM they were able to identify informative Tweets with better results compared with studies using more traditional input features, with F1 scores often above 80\%.

A notable issue with the use of these machine learning models for text classification is the reliance on feature representations as input, including bag of words, or statistical features like the proportion of upper case letters, bigrams, _et cetera_. This approach does not preserve word order, and approaches that take keyword inputs also risk over-fitting [@caragea2016]. Alternatively to these traditional machine learning methods, is the potential for neural networks, which in some cases are able to preserve word order, allowing for more accurate representations of the input text.

@caragea2016 directly compare the results of an SVM against a Convolutional Neural Network (CNN) for the classification and identification of informative Tweets relating to crisis events. Overall their CNN model outperforms several iterations of SVMs that each use various feature types.

Similarly, @lorini2019 use 'GloVe' word embeddings from as input to an CNN to classify Tweets relating to flooding events in multiple languages, as an alternative to other methods that use bag of word bigrams or unigrams. They note that the use of word embeddings has the ability to capture language specific semantic information that may be lost when using traditional methods.

@ghafarian2020 compare the results of an SVM for formative flood Tweet classification against a method proposed in @nguyen2017 that instead uses a CNN with pre-trained word embeddings. Overall performance of the CNN was notably better than the SVM, however @ghafarian2020 note that for very small dataset sizes, the SVM model performs better.

# Information Extraction from Tweets

## Geographic Entity Recognition

Geographic Information Retrieval (GIR) relates to the broad research area that considers the extraction of geographic information from text. This information primarily exists as place names in text, which are extractable using automated tools built using Named Entity Recognition (NER) models.

Recent progress in GIR research has presented models which target place names explicitly, excluding irrelevant entities which are often found with established corpora, traditionally used to train such models [@tjongkimsang2003;@mani2010]. Using NER to identify place names, rather than keyword matching using a gazetteer enables place names to be extracted that may not formally exist. These exist as nicknames, misspellings, and fine-grained place names, all of which are common on Twitter when referring to emergency events [@grace2020a].

## Temporal Information

Past studies have considered the delineation of events into their temporal stages, for example _before_, _during_, and _after_ [@iyengar2011;@kryvasheyeu2016], considering how the language shifts between these stages.

@kryvasheyeu2016 found that the per-capita number of Twitter messages corresponds directly with disaster-inflicted monetary damage, with more prominence following the peak of a disaster. They demonstrate that surface level analysis of social media may provide a suitable preliminary rapid damage assessment immediately following a disaster.

# Natural Language Processing

The first major developments in Natural Language Processing (NLP) derive from word embeddings. Word embeddings are an embedded representation of a word, typically a high dimensional vector of floats, mapped to a location in space using some form of unsupervised training procedure. The first word embeddings were generated by @mikolov2013 using an algorithm called Word2Vec. These embeddings were created from a large corpus of text using a shallow neural network, which enabled the embeddings of words used in similar contexts have similar representations in their corresponding vector space. Word2Vec however is limited by its speed, meaning it becomes inefficient with larger amounts of data, leading to embeddings that are overall limited in accuracy. This was in part solved by GloVe embeddings which, using an optimised algorithm, was able to speed up the creation of word embeddings, while achieving higher accuracy on word analogy and word similarity tasks through unsupervised training [@pennington2014].

More recent developments in NLP have come through the use of the _transformer_ architecture. @vaswani2017 demonstrated that the attention mechanism is alone suitable for many NLP tasks, moving models away from the purely sequential architectures built on Recurrent Neural Networks (RNNs). Transformers have several key benefits over previously common architectures like Long Short-Term Memory (LSTMs), as attention allows all tokens in a sequence to be weighted against each other, unlike LSTMs where the sequences are processed in order. This allows contextual information to propagate through each word, meaning embedded information relates to that token in its particular context, and not the non-specific learned embeddings derived from general text corpora from pre-trained algorithms like GloVe. This feature also means that transformers can be highly parallelised, allowing for much faster training times on GPUs, despite their larger size. Parallelisation has enabled pre-trained transformer models to be built, often known as _Language Models_ which describes their pre-training procedure. The most famous is BERT [@devlin2019], which used unsupervised training to learn word embeddings as model weights, using the full English language Wikipedia corpus and the Common Crawl. These language models are often fine-tuned to specific tasks like Named Entity Recognition (NER), where weights are updated on a task-specific labelled dataset, modifying the embedded weights that were learned during the pre-training procedure.

My dissertation proposes using the Twitter API v2 to extract Tweets that occurred during past severe flooding events in the UK, within the bounding box for the affected flood zone. This corpus emulates the expected baseline information that would be extractable automatically from Twitter during such events. Tweets will then be classified into _flood related_ and _unrelated_, removing noise that is present when extracting information from social media. This flood related corpus is then analysed with respect to its spatial and temporal information. This dissertation aims to demonstrate the ability to automatically extract _relevant_ flood related Tweets during known flood events in real-time to assist with emergency response.
