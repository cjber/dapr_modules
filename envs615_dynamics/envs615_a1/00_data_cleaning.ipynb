{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import pandas as pd\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clean_data(file: str):\n",
    "    \"\"\"Read in and clean the Kaggle car data.\n",
    "\n",
    "    Clean by removing outlier values from the various variables.\n",
    "    These values were identified prior to this analysis. Also\n",
    "    removes large number of columns that don't provide any useful\n",
    "    information.\n",
    "    Provides outputs, the clean file, and a boolean series giving integer\n",
    "    postions of dropped columns\n",
    "\n",
    "    Args:\n",
    "        file (str): the file and path location to read in\n",
    "    \"\"\"\n",
    "    # read in the data, special encoding required\n",
    "    df = pd.read_csv(file, encoding=\"latin-1\")\n",
    "    # remove some questionable values, due to large number of rows\n",
    "    # consider it fine to remove values that are possibly real but\n",
    "    # will not improve models\n",
    "    invalid_year = (df['yearOfRegistration'] > 2020) |\\\n",
    "                   (df['yearOfRegistration'] < 1950)\n",
    "    invalid_price = (df['price'] > 100000) |\\\n",
    "                    (df['price'] < 100)\n",
    "    invalid_power = (df['powerPS'] > 500) |\\\n",
    "                    (df['powerPS'] < 50)\n",
    "    # variable used to remove outliers\n",
    "    match = invalid_price | invalid_year | invalid_power\n",
    "    match_dropped = {\"Number of dropped rows:\": match.sum(),\n",
    "                     \"Percentage of dropped rows:\": (match.sum()/len(df))}\n",
    "    # drop matched rows, also remove columns that are not required\n",
    "    df = df[-match]\\\n",
    "        .drop([\n",
    "            'dateCrawled',\n",
    "            'seller',  # almost all one value\n",
    "            'offerType',  # almost all one value\n",
    "            'name',  # too many unique values\n",
    "            'abtest',  # uninterpretable ebay code\n",
    "            'lastSeen',\n",
    "            'nrOfPictures',  # all zero\n",
    "            'dateCreated',\n",
    "            'model'  # far too many values\n",
    "        ], axis=1)\n",
    "    return df, match_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, match_dropped = read_clean_data(\"./data/autos.csv\")\n",
    "match_dropped  # count of rows removed in cleaning\n",
    "# find number of rows with NA values\n",
    "print(\"Total NA Values:\", df.shape[0] - df.dropna().shape[0])\n",
    "df = df.dropna()  # drop NA rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_rows_de(df: pd.DataFrame, cols: list):\n",
    "    \"\"\"Translate selected rows in a dataframe from de to en.\n",
    "\n",
    "    Utilises googles translation API to accurately translate text\n",
    "    contained within a dataframe. For efficient use first the number of\n",
    "    unique values in each chosen column are found and translated. These\n",
    "    may then be added back into the dataframe using pandas.replace.\n",
    "    This avoids applying the translation API to each row, which does not\n",
    "    work with a large dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): a dataframe with values that require translation\n",
    "        cols (list): columns containing rows to translate\n",
    "    \"\"\"\n",
    "    # initialise google Translator object\n",
    "    translator = Translator()\n",
    "    translations = {}  # empty dict to store translations\n",
    "    # loop through each column specified above in the df\n",
    "    # find unique categorical values for each\n",
    "    # for each unique value per row translate from de to en\n",
    "    # store the translation in the dictionary\n",
    "    for column in cols:\n",
    "        # unique elements of the column\n",
    "        unique_elements = df[column].unique()\n",
    "        for element in unique_elements:\n",
    "            # add translation to the dictionary\n",
    "            translations[element] = translator.translate(\n",
    "                element, src='de', dest='en').text\n",
    "    df = df.replace(translations)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns that require translation from german\n",
    "cols = [\n",
    "    'vehicleType',\n",
    "    'notRepairedDamage',\n",
    "    'fuelType',\n",
    "    'gearbox'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run translation function on predetermined rows\n",
    "df = translate_rows_de(df, cols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one translation needs correcting\n",
    "df = df\\\n",
    "    .replace('manually', 'manual')\n",
    "df = df.rename(columns={'notRepairedDamage': 'damaged'})  # rename for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_age(df: pd.DataFrame, year_col: str, month_col: str):\n",
    "    \"\"\"Convert the dataframe to work with date manipulation.\n",
    "\n",
    "    Both *_col inputs must evaluate to a column found within the\n",
    "    dataframe, with a year and month in the correct format.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): [TODO:description]\n",
    "        year_col (str): year in the format: 2020\n",
    "        month_col (str): month in the format: 01\n",
    "    \"\"\"\n",
    "    # create date type from month and year of registration\n",
    "    date_reg = df.rename(\n",
    "        columns={\n",
    "            year_col: 'year',\n",
    "            month_col: 'month'\n",
    "        }\n",
    "    )[['year', 'month']]\n",
    "    date_reg['day'] = '1'  # required for to_datetime\n",
    "    # change to date class, coerce removes invalid dates\n",
    "    date_reg = pd.to_datetime(date_reg, errors='coerce')\n",
    "    # subtract date from current date to find age\n",
    "\n",
    "    # run function days_subtract on all rows\n",
    "    date_reg = date_reg.apply(lambda x: pd.to_datetime(date.today()) - x)\n",
    "    df['age'] = date_reg.dt.days/365  # age in years\n",
    "    df = df.drop([year_col, month_col], axis=1)  # remove old cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = find_age(\n",
    "    df,\n",
    "    year_col='yearOfRegistration',\n",
    "    month_col='monthOfRegistration'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invalid dates are now classified as NA, remove them\n",
    "# find number of rows with NA values\n",
    "print(\"Number of NA Values:\", df.shape[0] - df.dropna().shape[0])\n",
    "df = df.dropna()  # drop NA rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_postcodes(df: pd.DataFrame, pc: str):\n",
    "    # read in detailed postcode data from GeoNames\n",
    "    # keep only largest authority district\n",
    "    # remove duplicate rows (since smaller authorities are gone)\n",
    "    pc_df = pd.read_csv(pc, sep=\"\\t\", header=None)\\\n",
    "        .iloc[:, [1, 3]]\\\n",
    "        .drop_duplicates()\n",
    "    # rename for join\n",
    "    pc_df.columns = ['postalCode', 'area']\n",
    "    # setup a join to include new area column with associated postcode\n",
    "    df = df.set_index('postalCode')\\\n",
    "        .join(pc_df.set_index('postalCode'))\\\n",
    "        .reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE postcodes from GeoNames\n",
    "df = join_postcodes(df, \"./data/DE.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improved category, far fewer unique values\n",
    "# number of unique postcodes\n",
    "len(df['postalCode'].unique())\n",
    "# number of unique areas\n",
    "len(df['area'].unique())\n",
    "df = df.drop('postalCode', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/derived/cars_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
